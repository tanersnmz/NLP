# NLP Homework Assignments

This repository contains solutions to the NLP assignments for Fall 2024. Each assignment builds on concepts in natural language processing, including language modeling, dependency parsing, image captioning, and fine-tuning BERT. Below is an overview of each homework and its objectives.

## Homework 1: Trigram Language Model
- **Objective:** Build a trigram language model to extract, count, and compute n-gram probabilities.
- **Key Tasks:**
  - Implement functions to extract n-grams from text and count occurrences in a corpus.
  - Calculate raw and smoothed probabilities.
  - Compute sentence probabilities and perplexity for evaluation.
  - Apply the model to text classification for essay scoring based on perplexity.


## Homework 2: Dependency Parsing with Neural Networks
- **Objective:** Train a neural network to predict transitions for an arc-standard dependency parser.
- **Key Tasks:**
  - Design input/output representations for training the neural network.
  - Define the neural network architecture using PyTorch.
  - Train the model and implement a greedy parsing algorithm.
  - Evaluate the parser using labeled and unlabeled attachment scores.


## Homework 3: Image Captioning with a Conditioned LSTM Generator
- **Objective:** Train an LSTM model for image captioning.
- **Key Tasks:**
  - Preprocess image and text data.
  - Design and train a conditioned LSTM generator.
  - Evaluate the model's performance using BLEU scores.


## Homework 4: Fine-tuning BERT for Semantic Role Labeling (SRL)
- **Objective:** Fine-tune a BERT model for SRL tasks.
- **Key Tasks:**
  - Preprocess the OntoNotes SRL dataset.
  - Fine-tune BERT for token classification.
  - Evaluate model performance on SRL tasks.

## License
This repository is licensed for educational use only, following the guidelines of Columbia University's NLP course.
